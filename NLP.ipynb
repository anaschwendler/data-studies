{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estudando NLP\n",
    "\n",
    "O que eu pretendo usar: CSTNews Corpus\n",
    "\n",
    "\n",
    "NLTK;\n",
    "\n",
    "Portuguese Examples: http://www.nltk.org/howto/portuguese_en.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tokenização\n",
    "\n",
    "## 1.1 Tokenização em setenças\n",
    "\n",
    "Token é um pedaço de um todo, então: \n",
    "\n",
    "- uma palavra é um token em uma sentença;\n",
    "- uma sentença é um token em um paragrafo.\n",
    "\n",
    "Logo separar um paragrafo em sentenças é tokenizar sentenças."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Um acidente aéreo na localidade de Bukavu, no leste da República Democrática do Congo, matou 17 pessoas na quinta-feira à tarde, informou hoje um porta-voz das Nações Unidas.', 'As vítimas do acidente foram 14 passageiros e três membros da tripulação.', 'Todos morreram quando o avião, prejudicado pelo mau tempo, não conseguiu chegar à pista de aterrissagem e caiu numa floresta a 15 Km do aeroporto de Bukavu.', 'O avião explodiu e se incendiou, acrescentou o porta-voz da ONU em Kinshasa, Jean-Tobias Okala.', '\"Não houve sobreviventes\", disse Okala.', 'O porta-voz informou que o avião, um Soviet Antonov-28 de fabricação ucraniana e propriedade de uma companhia congolesa, a Trasept Congo, também levava uma carga de minerais.', 'Segundo fontes aeroportuárias, os membros da tripulação eram de nacionalidade russa.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "f = open('sample.txt')\n",
    "dataset = f.read()\n",
    "\n",
    "sentence_tokenized = sent_tokenize(dataset)\n",
    "print(sentence_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenização em Português!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Um acidente aéreo na localidade de Bukavu, no leste da República Democrática do Congo, matou 17 pessoas na quinta-feira à tarde, informou hoje um porta-voz das Nações Unidas.',\n",
       " 'As vítimas do acidente foram 14 passageiros e três membros da tripulação.',\n",
       " 'Todos morreram quando o avião, prejudicado pelo mau tempo, não conseguiu chegar à pista de aterrissagem e caiu numa floresta a 15 Km do aeroporto de Bukavu.',\n",
       " 'O avião explodiu e se incendiou, acrescentou o porta-voz da ONU em Kinshasa, Jean-Tobias Okala.',\n",
       " '\"Não houve sobreviventes\", disse Okala.',\n",
       " 'O porta-voz informou que o avião, um Soviet Antonov-28 de fabricação ucraniana e propriedade de uma companhia congolesa, a Trasept Congo, também levava uma carga de minerais.',\n",
       " 'Segundo fontes aeroportuárias, os membros da tripulação eram de nacionalidade russa.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.data\n",
    "portuguese_tokenizer = nltk.data.load('tokenizers/punkt/PY3/portuguese.pickle')\n",
    "portuguese_tokenizer.tokenize(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Tokenização em palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Um', 'acidente', 'aéreo', 'na', 'localidade', 'de', 'Bukavu', ',', 'no', 'leste', 'da', 'República', 'Democrática', 'do', 'Congo', ',', 'matou', '17', 'pessoas', 'na', 'quinta-feira', 'à', 'tarde', ',', 'informou', 'hoje', 'um', 'porta-voz', 'das', 'Nações', 'Unidas', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "first_sentence_word_tokenized = word_tokenize(sentence_tokenized[0])\n",
    "print(first_sentence_word_tokenized)## 1.3 Tokenização com expressão regular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Tokenização com expressão regular\n",
    "\n",
    "Por exemplo para pegar apenas palavras em um texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Um', 'acidente', 'aéreo', 'na', 'localidade', 'de', 'Bukavu', 'no', 'leste', 'da', 'República', 'Democrática', 'do', 'Congo', 'matou', '17', 'pessoas', 'na', 'quinta', 'feira', 'à', 'tarde', 'informou', 'hoje', 'um', 'porta', 'voz', 'das', 'Nações', 'Unidas']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "\n",
    "first_sentence_word_tokenized_without_punctuation = tokenizer.tokenize(sentence_tokenized[0])\n",
    "print(first_sentence_word_tokenized_without_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Treinando um tokenizador de sentenças\n",
    "\n",
    "O tokenizador do NLTK é para uso geral, nem sempre é a melhor opção para textos, dependendo da formatação do texto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.corpus import webtext\n",
    "\n",
    "text = webtext.raw('overheard.txt')\n",
    "sent_tokenizer = PunktSentenceTokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['White guy: So, do you have any plans for this evening?',\n",
       " 'Asian girl: Yeah, being angry!',\n",
       " 'White guy: Oh, that sounds good.',\n",
       " 'Guy #1: So this Jack guy is basically the luckiest man in the world.',\n",
       " \"Guy #2: Why, because he's survived like 5 attempts on his life and it's not even noon?\",\n",
       " 'Guy #1: No; he could totally nail those two chicks.',\n",
       " 'Dad: Could you tell me where the auditorium is?',\n",
       " \"Security guy: It's on the second floor.\",\n",
       " \"Dad: Wait, you mean it's actually in the building?\",\n",
       " \"Girl: But, I mean, it's not like I ever plan on giving birth.\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = sent_tokenizer.tokenize(text)\n",
    "sents[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observador, as sentenças são separadas (tokenizadas) em conversas, por que o `PunktSentenceTokenizer` usa um algorítmo de unsupervised learning para aprender o que constitui uma quebra de sentença. É não-supervisionado por que você não precisa dar nenhum texto para treinamento do algoritmo, apenas o texto em si."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Filtrando stopwords\n",
    "\n",
    "Stopwords são palavras que geralmente não contribuem para o significado de uma sentença."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Um', 'acidente', 'aéreo', 'localidade', 'Bukavu', 'leste', 'República', 'Democrática', 'Congo', 'matou', '17', 'pessoas', 'quinta', 'feira', 'tarde', 'informou', 'hoje', 'porta', 'voz', 'Nações', 'Unidas']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "portuguese_stops = set(stopwords.words('portuguese'))\n",
    "words = first_sentence_word_tokenized_without_punctuation\n",
    "\n",
    "words_without_stop = [word for word in words if word not in portuguese_stops]\n",
    "print(words_without_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Substituindo e corrigindo palavras\n",
    "\n",
    "## 1.1 Stemming \n",
    "\n",
    "Stemming é a técnica que remove os afixos das palavras, deixando apenas seu radical, existe uma versão em Português que é `RSLPStemmer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acid\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RSLPStemmer\n",
    "\n",
    "stemmer = RSLPStemmer()\n",
    "stem_acidente = stemmer.stem(words_without_stop[1]) #acidente\n",
    "print(stem_acidente)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outro stemmer que pode ser usado é o SnowballStemmer, que tem opção pt-br:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'acident'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stem_acidente = SnowballStemmer('portuguese')\n",
    "stem_acidente.stem(words_without_stop[1]) #acidente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os resultados foram diferentes como podemos notar, então vale ver qual a aplicação, para decidir qual é o melhor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Transformando texto raw no formato do NLTK\n",
    "\n",
    "NLTK tem seu formato de texto padrão, para converter o que é preciso ser feito:\n",
    "\n",
    "- Tokenizar o texto em palavras\n",
    "- Usar método de conversão para `nltk.txt``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Tokenizando em palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokenized = word_tokenize(dataset)\n",
    "type(text_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Convertendo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.text.Text"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_format = nltk.Text(text_tokenized)\n",
    "type(text_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By transforming a raw text into a text we can use more NLTK features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise de estrutura de texto\n",
    "\n",
    "A concordância nos permite ver palavras em contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 2 of 2 matches:\n",
      "                                   acidente aéreo na localidade de Bukavu , no\n",
      " das Nações Unidas . As vítimas do acidente foram 14 passageiros e três membro\n"
     ]
    }
   ],
   "source": [
    "text_format.concordance('acidente')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_format.similar('um')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_format.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Convertendo palavras combinando com expressões regulares\n",
    "\n",
    "Essa combinação é boa para converter palavras que foram diminuídas e problemas de linguagem. \n",
    "\n",
    "Por exemplo: 'Pq' > 'porque', 'Coé' > 'Qual é'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata \n",
    "\n",
    "replacement_patterns = [\n",
    "    (r'pq', 'porque'),\n",
    "    (r'coe', 'qual e'),\n",
    "    (r'vc', 'voce'),\n",
    "]\n",
    "    \n",
    "def normalize_string(string):\n",
    "    if isinstance(string, str):\n",
    "        nfkd_form = unicodedata.normalize('NFKD', string.lower())\n",
    "        return nfkd_form.encode('ASCII', 'ignore').decode('utf-8')\n",
    "\n",
    "def replace(text, patterns=replacement_patterns):\n",
    "    s = normalize_string(text)\n",
    "    patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "\n",
    "    for (pattern, repl) in patterns:\n",
    "        s = re.sub(pattern, repl, s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qual e, porque voce fez isso?\n"
     ]
    }
   ],
   "source": [
    "print(replace(\"Coé, pq vc fez isso?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: Enchant\n",
    "\n",
    "Existe uma biblioteca que faz correção ortográfica chamada (Enchant)[http://pythonhosted.org/pyenchant/tutorial.html], mas ela não possui dicionário em português:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de_DE', 'fr_FR', 'en_GB', 'en_AU', 'en_US']\n"
     ]
    }
   ],
   "source": [
    "import enchant\n",
    "print(enchant.list_languages())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análisando a frequência relativa de palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Um': 1, 'acidente': 2, 'aéreo': 1, 'na': 2, 'localidade': 1, 'de': 7, 'Bukavu': 2, ',': 12, 'no': 1, 'leste': 1, 'da': 4, 'República': 1, 'Democrática': 1, 'do': 3, 'Congo': 2, 'matou': 1, '17': 1, 'pessoas': 1, 'quinta-feira': 1, 'à': 2, 'tarde': 1, 'informou': 2, 'hoje': 1, 'um': 2, 'porta-voz': 3, 'das': 1, 'Nações': 1, 'Unidas': 1, '.': 7, 'As': 1, 'vítimas': 1, 'foram': 1, '14': 1, 'passageiros': 1, 'e': 4, 'três': 1, 'membros': 2, 'tripulação': 2, 'Todos': 1, 'morreram': 1, 'quando': 1, 'o': 3, 'avião': 3, 'prejudicado': 1, 'pelo': 1, 'mau': 1, 'tempo': 1, 'não': 1, 'conseguiu': 1, 'chegar': 1, 'pista': 1, 'aterrissagem': 1, 'caiu': 1, 'numa': 1, 'floresta': 1, 'a': 2, '15': 1, 'Km': 1, 'aeroporto': 1, 'O': 2, 'explodiu': 1, 'se': 1, 'incendiou': 1, 'acrescentou': 1, 'ONU': 1, 'em': 1, 'Kinshasa': 1, 'Jean-Tobias': 1, 'Okala': 2, '``': 1, 'Não': 1, 'houve': 1, 'sobreviventes': 1, \"''\": 1, 'disse': 1, 'que': 1, 'Soviet': 1, 'Antonov-28': 1, 'fabricação': 1, 'ucraniana': 1, 'propriedade': 1, 'uma': 2, 'companhia': 1, 'congolesa': 1, 'Trasept': 1, 'também': 1, 'levava': 1, 'carga': 1, 'minerais': 1, 'Segundo': 1, 'fontes': 1, 'aeroportuárias': 1, 'os': 1, 'eram': 1, 'nacionalidade': 1, 'russa': 1}\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "fd1 = FreqDist(text_format)\n",
    "print(dict(fd1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
